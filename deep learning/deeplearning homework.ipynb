{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOM/aNbDie9PU3pi9zrMKV9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","# NECESSARY IMPORTS"],"metadata":{"id":"GPwhRzZr3WAG"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":117,"status":"ok","timestamp":1764778192927,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"},"user_tz":-180},"id":"9CP4TI6bMw7Q","outputId":"d47f77f8-ad80-4dc6-e643-a1d02153beab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.12.12\n"]}],"source":["!python --version\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"CPl3aVzFqiss","executionInfo":{"status":"ok","timestamp":1764779351894,"user_tz":-180,"elapsed":579130,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"}},"outputId":"60995cce-299e-4f0e-adf3-18b290b98cd1"},"source":["!wget https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n","!unzip data.zip"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-12-03 16:19:32--  https://github.com/SVizor42/ML_Zoomcamp/releases/download/straight-curly-data/data.zip\n","Resolving github.com (github.com)... 140.82.113.3\n","Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://release-assets.githubusercontent.com/github-production-release-asset/405934815/e712cf72-f851-44e0-9c05-e711624af985?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-03T17%3A04%3A31Z&rscd=attachment%3B+filename%3Ddata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-03T16%3A04%3A11Z&ske=2025-12-03T17%3A04%3A31Z&sks=b&skv=2018-11-09&sig=76KkiYcw2L7ytvVwJJwa7O1sIxDlFhLSwiu1rdU0%2B9o%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDc4MDU3MiwibmJmIjoxNzY0Nzc4NzcyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.3SijbqbSqdMMqFtzORKu0cEtwkd7rHe9v1OtmYXlPzg&response-content-disposition=attachment%3B%20filename%3Ddata.zip&response-content-type=application%2Foctet-stream [following]\n","--2025-12-03 16:19:32--  https://release-assets.githubusercontent.com/github-production-release-asset/405934815/e712cf72-f851-44e0-9c05-e711624af985?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-12-03T17%3A04%3A31Z&rscd=attachment%3B+filename%3Ddata.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-12-03T16%3A04%3A11Z&ske=2025-12-03T17%3A04%3A31Z&sks=b&skv=2018-11-09&sig=76KkiYcw2L7ytvVwJJwa7O1sIxDlFhLSwiu1rdU0%2B9o%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2NDc4MDU3MiwibmJmIjoxNzY0Nzc4NzcyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.3SijbqbSqdMMqFtzORKu0cEtwkd7rHe9v1OtmYXlPzg&response-content-disposition=attachment%3B%20filename%3Ddata.zip&response-content-type=application%2Foctet-stream\n","Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 102516572 (98M) [application/octet-stream]\n","Saving to: ‘data.zip.3’\n","\n","data.zip.3          100%[===================>]  97.77M   288MB/s    in 0.3s    \n","\n","2025-12-03 16:19:33 (288 MB/s) - ‘data.zip.3’ saved [102516572/102516572]\n","\n","Archive:  data.zip\n","replace data/test/curly/03312ac556a7d003f7570657f80392c34.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "]}]},{"cell_type":"markdown","source":[" PRINT THE TOTAL NUMBER OF PARAMETER IN THE  MODEL"],"metadata":{"id":"BBlbACG63xBr"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9bebedbf","executionInfo":{"status":"ok","timestamp":1764779370420,"user_tz":-180,"elapsed":15,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"}},"outputId":"f8f0a04f-700c-48b5-8598-1bbd03498958"},"source":["print(f\"Loss function used: {model.loss}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss function used: binary_crossentropy\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rogpgXEKw67L"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import load_img\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fe543507","executionInfo":{"status":"ok","timestamp":1764779449547,"user_tz":-180,"elapsed":15,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"}},"outputId":"b3eef7ef-6c7f-42f6-aedd-69b1d9d94728"},"source":["np.random.seed(42)\n","tf.random.set_seed(42)\n","print(\"Random seeds set for NumPy and TensorFlow.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Random seeds set for NumPy and TensorFlow.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"58171998","executionInfo":{"status":"ok","timestamp":1764779456874,"user_tz":-180,"elapsed":41,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"}},"outputId":"20917bba-db57-4d37-c6f0-c3050343559c"},"source":["TARGET_SIZE = (200, 200)\n","BATCH_SIZE = 32\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    validation_split=0.2\n",")\n","test_datagen = ImageDataGenerator(\n","    rescale=1./255\n",")\n","\n","train_generator = train_datagen.flow_from_directory(\n","    'data/train',\n","    target_size=TARGET_SIZE,\n","    batch_size=BATCH_SIZE,\n","    class_mode='binary',\n","    subset='training',\n","    seed=42\n",")\n","validation_generator = train_datagen.flow_from_directory(\n","    'data/train',\n","    target_size=TARGET_SIZE,\n","    batch_size=BATCH_SIZE,\n","    class_mode='binary',\n","    subset='validation',\n","    seed=42\n",")\n","test_generator = test_datagen.flow_from_directory(\n","    'data/test',\n","    target_size=TARGET_SIZE,\n","    batch_size=BATCH_SIZE,\n","    class_mode='binary',\n","    seed=42,\n","    shuffle=False\n",")\n","\n","print(\"Data generators and flow_from_directory successful.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 640 images belonging to 2 classes.\n","Found 160 images belonging to 2 classes.\n","Found 201 images belonging to 2 classes.\n","Data generators and flow_from_directory successful.\n"]}]},{"cell_type":"markdown","metadata":{"id":"634b57b7"},"source":[" CNN Model Architecture    we  also see the total nmber of parameters of the model\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"9d397529","executionInfo":{"status":"ok","timestamp":1764779464144,"user_tz":-180,"elapsed":384,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"}},"outputId":"227b0665-efe8-4985-9801-5eddd885fdb5"},"source":["model = Sequential([\n","    Conv2D(32, (3, 3), activation='relu', input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 3)),\n","    MaxPooling2D((2, 2)),\n","    Flatten(),\n","    Dense(64, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.summary()\n","print(\"CNN model architecture defined successfully.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_2\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m313632\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │    \u001b[38;5;34m20,072,512\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">313632</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │    <span style=\"color: #00af00; text-decoration-color: #00af00\">20,072,512</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,073,473\u001b[0m (76.57 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,073,473</span> (76.57 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,073,473\u001b[0m (76.57 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,073,473</span> (76.57 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["CNN model architecture defined successfully.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3df448db","executionInfo":{"status":"ok","timestamp":1764778772289,"user_tz":-180,"elapsed":4,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"}},"outputId":"860246ca-5ba6-4b51-cceb-21bc0082ac1e"},"source":["model.compile(\n","    optimizer=Adam(learning_rate=0.001),\n","    loss='binary_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","print(\"Model compiled successfully.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model compiled successfully.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"97d71d48","executionInfo":{"status":"ok","timestamp":1764779752198,"user_tz":-180,"elapsed":39,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"}},"outputId":"b5a06c5d-eedf-4df1-839e-6a4635d3412d"},"source":["import torchvision.transforms as transforms\n","\n","# Define the transformations for training data\n","train_transform = transforms.Compose([\n","    transforms.Resize(TARGET_SIZE),\n","    transforms.CenterCrop(TARGET_SIZE),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Define the transformations for test data\n","test_transform = transforms.Compose([\n","    transforms.Resize(TARGET_SIZE),\n","    transforms.CenterCrop(TARGET_SIZE),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","print(\"PyTorch data transformations defined successfully.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch data transformations defined successfully.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"60c2660e","executionInfo":{"status":"ok","timestamp":1764779777709,"user_tz":-180,"elapsed":62,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"}},"outputId":"909bff5b-07a7-4fdb-bb87-5110d23bfcf2"},"source":["import torchvision.datasets as datasets\n","import torch.utils.data as data\n","\n","BATCH_SIZE_PYTORCH = 20\n","\n","# Create datasets\n","train_dataset = datasets.ImageFolder('data/train', transform=train_transform)\n","validation_dataset = datasets.ImageFolder('data/train', transform=train_transform)\n","test_dataset = datasets.ImageFolder('data/test', transform=test_transform)\n","\n","# Create DataLoaders\n","train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE_PYTORCH, shuffle=True)\n","validation_loader = data.DataLoader(validation_dataset, batch_size=BATCH_SIZE_PYTORCH, shuffle=True)\n","test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE_PYTORCH, shuffle=False)\n","\n","print(f\"Number of images in training dataset: {len(train_dataset)}\")\n","print(f\"Number of images in validation dataset: {len(validation_dataset)}\")\n","print(f\"Number of images in test dataset: {len(test_dataset)}\")\n","print(\"PyTorch DataLoaders re-created successfully with updated transformations.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images in training dataset: 800\n","Number of images in validation dataset: 800\n","Number of images in test dataset: 201\n","PyTorch DataLoaders re-created successfully with updated transformations.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"067c351a","executionInfo":{"status":"ok","timestamp":1764781504402,"user_tz":-180,"elapsed":741833,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"}},"outputId":"d43a2c6b-5d4f-40ec-8c42-78fbfb1346e1"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# 1. Define the CNN Model Architecture\n","class CNNModel(nn.Module):\n","    def __init__(self):\n","        super(CNNModel, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3), stride=1, padding=0)\n","        self.relu1 = nn.ReLU()\n","\n","        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n","\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(32 * 99 * 99, 64)\n","        self.relu2 = nn.ReLU()\n","\n","        self.fc2 = nn.Linear(64, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.relu1(x)\n","        x = self.pool1(x)\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.relu2(x)\n","        x = self.fc2(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","# 2. Instantiate the CNNModel and move to device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","model_pytorch = CNNModel().to(device)\n","print(\"PyTorch CNN model instantiated and moved to device.\")\n","\n","# 3. Define Loss Function and Optimizer\n","criterion = nn.BCELoss()\n","optimizer_pytorch = optim.Adam(model_pytorch.parameters(), lr=0.001)\n","print(\"Loss function (BCELoss) and optimizer (Adam) defined.\")\n","\n","# 4. Initialize history dictionary\n","pytorch_history = {\n","    'train_loss': [],\n","    'train_accuracy': [],\n","    'val_loss': [],\n","    'val_accuracy': []\n","}\n","\n","# 5. Set number of epochs\n","num_epochs_pytorch = 10\n","print(f\"Number of epochs set to: {num_epochs_pytorch}\")\n","\n","# Training and Validation Loop\n","print(\"Starting PyTorch training loop...\")\n","for epoch in range(num_epochs_pytorch):\n","    model_pytorch.train()\n","    running_train_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        labels = labels.float().unsqueeze(1)\n","\n","        optimizer_pytorch.zero_grad()\n","        outputs = model_pytorch(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer_pytorch.step()\n","\n","        running_train_loss += loss.item() * images.size(0)\n","        predicted = (outputs > 0.5).float()\n","        correct_train += (predicted == labels).sum().item()\n","        total_train += labels.size(0)\n","\n","    avg_train_loss = running_train_loss / len(train_dataset)\n","    train_accuracy = correct_train / total_train\n","\n","    # Validation Loop\n","    model_pytorch.eval()\n","    running_val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","\n","    with torch.no_grad():\n","        for images, labels in validation_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            labels = labels.float().unsqueeze(1)\n","\n","            outputs = model_pytorch(images)\n","            loss = criterion(outputs, labels)\n","\n","            running_val_loss += loss.item() * images.size(0)\n","            predicted = (outputs > 0.5).float()\n","            correct_val += (predicted == labels).sum().item()\n","            total_val += labels.size(0)\n","\n","    avg_val_loss = running_val_loss / len(validation_dataset)\n","    val_accuracy = correct_val / total_val\n","\n","    # Store metrics\n","    pytorch_history['train_loss'].append(avg_train_loss)\n","    pytorch_history['train_accuracy'].append(train_accuracy)\n","    pytorch_history['val_loss'].append(avg_val_loss)\n","    pytorch_history['val_accuracy'].append(val_accuracy)\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs_pytorch}], ' \\\n","          f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, ' \\\n","          f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n","\n","print(\"PyTorch training loop finished.\")\n","\n","# Evaluate on Test Dataset\n","print(\"Evaluating PyTorch model on test dataset...\")\n","model_pytorch.eval()\n","test_loss = 0.0\n","correct_test = 0\n","total_test = 0\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        labels = labels.float().unsqueeze(1)\n","\n","        outputs = model_pytorch(images)\n","        loss = criterion(outputs, labels)\n","\n","        test_loss += loss.item() * images.size(0)\n","        predicted = (outputs > 0.5).float()\n","        correct_test += (predicted == labels).sum().item()\n","        total_test += labels.size(0)\n","\n","final_test_loss = test_loss / len(test_dataset)\n","final_test_accuracy = correct_test / total_test\n","\n","print(f'Test Loss: {final_test_loss:.4f}, Test Accuracy: {final_test_accuracy:.4f}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","PyTorch CNN model instantiated and moved to device.\n","Loss function (BCELoss) and optimizer (Adam) defined.\n","Number of epochs set to: 10\n","Starting PyTorch training loop...\n","Epoch [1/10], Train Loss: 28.4541, Train Acc: 0.5975, Val Loss: 27.5954, Val Acc: 0.6637\n","Epoch [2/10], Train Loss: 29.6999, Train Acc: 0.6438, Val Loss: 21.0681, Val Acc: 0.6687\n","Epoch [3/10], Train Loss: 27.9098, Train Acc: 0.6562, Val Loss: 41.6578, Val Acc: 0.5563\n","Epoch [4/10], Train Loss: 24.2794, Train Acc: 0.7037, Val Loss: 18.2752, Val Acc: 0.7312\n","Epoch [5/10], Train Loss: 20.8840, Train Acc: 0.7175, Val Loss: 14.4333, Val Acc: 0.7600\n","Epoch [6/10], Train Loss: 16.0504, Train Acc: 0.7512, Val Loss: 9.8445, Val Acc: 0.8075\n","Epoch [7/10], Train Loss: 5.5927, Train Acc: 0.7887, Val Loss: 5.2021, Val Acc: 0.8113\n","Epoch [8/10], Train Loss: 1.7358, Train Acc: 0.8600, Val Loss: 0.2134, Val Acc: 0.9313\n","Epoch [9/10], Train Loss: 0.2148, Train Acc: 0.9425, Val Loss: 0.1216, Val Acc: 0.9537\n","Epoch [10/10], Train Loss: 0.0777, Train Acc: 0.9675, Val Loss: 0.0166, Val Acc: 0.9988\n","PyTorch training loop finished.\n","Evaluating PyTorch model on test dataset...\n","Test Loss: 0.9494, Test Accuracy: 0.7562\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2da7159c","executionInfo":{"status":"ok","timestamp":1764781508221,"user_tz":-180,"elapsed":7,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"}},"outputId":"7fd20148-1be3-43c8-d0f0-2858855d57e8"},"source":["import torchvision.transforms as transforms\n","\n","# Define the transformations for training data with augmentations\n","train_transform = transforms.Compose([\n","    transforms.RandomRotation(50),\n","    transforms.RandomResizedCrop(TARGET_SIZE[0], scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Define the transformations for test data (unchanged)\n","test_transform = transforms.Compose([\n","    transforms.Resize(TARGET_SIZE),\n","    transforms.CenterCrop(TARGET_SIZE),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","print(\"PyTorch data transformations updated with augmentations for training data.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch data transformations updated with augmentations for training data.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2b42bba7","executionInfo":{"status":"ok","timestamp":1764781538126,"user_tz":-180,"elapsed":24,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"}},"outputId":"cd9030bc-3bcb-4c69-8dc8-9f0de0688f34"},"source":["import torchvision.datasets as datasets\n","import torch.utils.data as data\n","\n","BATCH_SIZE_PYTORCH = 20\n","\n","# Create datasets\n","train_dataset = datasets.ImageFolder('data/train', transform=train_transform)\n","validation_dataset = datasets.ImageFolder('data/train', transform=test_transform)\n","test_dataset = datasets.ImageFolder('data/test', transform=test_transform)\n","\n","# Create DataLoaders\n","train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE_PYTORCH, shuffle=True)\n","validation_loader = data.DataLoader(validation_dataset, batch_size=BATCH_SIZE_PYTORCH, shuffle=True)\n","test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE_PYTORCH, shuffle=False)\n","\n","print(f\"Number of images in training dataset: {len(train_dataset)}\")\n","print(f\"Number of images in validation dataset: {len(validation_dataset)}\")\n","print(f\"Number of images in test dataset: {len(test_dataset)}\")\n","print(\"PyTorch DataLoaders re-created successfully with updated transformations.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images in training dataset: 800\n","Number of images in validation dataset: 800\n","Number of images in test dataset: 201\n","PyTorch DataLoaders re-created successfully with updated transformations.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1c64ca30","executionInfo":{"status":"ok","timestamp":1764782380408,"user_tz":-180,"elapsed":806264,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"}},"outputId":"d3d234f9-eb22-42f2-fd13-89a7de5ed45c"},"source":["pytorch_history_augmented = {\n","    'train_loss': [],\n","    'train_accuracy': [],\n","    'val_loss': [],\n","    'val_accuracy': [],\n","    'test_loss': [],\n","    'test_accuracy': []\n","}\n","\n","num_epochs_augmented = 10\n","print(f\"Number of additional epochs set to: {num_epochs_augmented}\")\n","\n","print(\"Starting PyTorch training loop with augmented data...\")\n","for epoch in range(num_epochs_augmented):\n","    # Training Loop\n","    model_pytorch.train()\n","    running_train_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","        labels = labels.float().unsqueeze(1)\n","\n","        optimizer_pytorch.zero_grad()\n","        outputs = model_pytorch(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer_pytorch.step()\n","\n","        running_train_loss += loss.item() * images.size(0)\n","        predicted = (outputs > 0.5).float()\n","        correct_train += (predicted == labels).sum().item()\n","        total_train += labels.size(0)\n","\n","    avg_train_loss = running_train_loss / len(train_dataset)\n","    train_accuracy = correct_train / total_train\n","\n","    # Validation Loop\n","    model_pytorch.eval()\n","    running_val_loss = 0.0\n","    correct_val = 0\n","    total_val = 0\n","\n","    with torch.no_grad():\n","        for images, labels in validation_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            labels = labels.float().unsqueeze(1)\n","\n","            outputs = model_pytorch(images)\n","            loss = criterion(outputs, labels)\n","\n","            running_val_loss += loss.item() * images.size(0)\n","            predicted = (outputs > 0.5).float()\n","            correct_val += (predicted == labels).sum().item()\n","            total_val += labels.size(0)\n","\n","    avg_val_loss = running_val_loss / len(validation_dataset)\n","    val_accuracy = correct_val / total_val\n","\n","    # Test Loop\n","    running_test_loss = 0.0\n","    correct_test = 0\n","    total_test = 0\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            labels = labels.float().unsqueeze(1)\n","\n","            outputs = model_pytorch(images)\n","            loss = criterion(outputs, labels)\n","\n","            running_test_loss += loss.item() * images.size(0)\n","            predicted = (outputs > 0.5).float()\n","            correct_test += (predicted == labels).sum().item()\n","            total_test += labels.size(0)\n","\n","    avg_test_loss = running_test_loss / len(test_dataset)\n","    test_accuracy = correct_test / total_test\n","\n","    # Store metrics\n","    pytorch_history_augmented['train_loss'].append(avg_train_loss)\n","    pytorch_history_augmented['train_accuracy'].append(train_accuracy)\n","    pytorch_history_augmented['val_loss'].append(avg_val_loss)\n","    pytorch_history_augmented['val_accuracy'].append(val_accuracy)\n","    pytorch_history_augmented['test_loss'].append(avg_test_loss)\n","    pytorch_history_augmented['test_accuracy'].append(test_accuracy)\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs_augmented}], ' \\\n","          f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, ' \\\n","          f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}, ' \\\n","          f'Test Loss: {avg_test_loss:.4f}, Test Acc: {test_accuracy:.4f}')\n","\n","print(\"PyTorch training loop with augmented data finished.\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of additional epochs set to: 10\n","Starting PyTorch training loop with augmented data...\n","Epoch [1/10], Train Loss: 0.8145, Train Acc: 0.7212, Val Loss: 0.0642, Val Acc: 0.9862, Test Loss: 0.5894, Test Acc: 0.7861\n","Epoch [2/10], Train Loss: 1.0405, Train Acc: 0.6913, Val Loss: 0.2021, Val Acc: 0.9200, Test Loss: 0.7210, Test Acc: 0.7264\n","Epoch [3/10], Train Loss: 0.5253, Train Acc: 0.7850, Val Loss: 0.1545, Val Acc: 0.9712, Test Loss: 0.4844, Test Acc: 0.7662\n","Epoch [4/10], Train Loss: 0.4649, Train Acc: 0.7875, Val Loss: 0.2188, Val Acc: 0.9263, Test Loss: 0.5518, Test Acc: 0.7413\n","Epoch [5/10], Train Loss: 0.4596, Train Acc: 0.7900, Val Loss: 0.2004, Val Acc: 0.9625, Test Loss: 0.5039, Test Acc: 0.7711\n","Epoch [6/10], Train Loss: 0.4466, Train Acc: 0.8087, Val Loss: 0.2445, Val Acc: 0.8988, Test Loss: 0.5701, Test Acc: 0.7413\n","Epoch [7/10], Train Loss: 0.4282, Train Acc: 0.7950, Val Loss: 0.1676, Val Acc: 0.9625, Test Loss: 0.4779, Test Acc: 0.7761\n","Epoch [8/10], Train Loss: 0.4162, Train Acc: 0.8313, Val Loss: 0.1681, Val Acc: 0.9587, Test Loss: 0.4573, Test Acc: 0.7861\n","Epoch [9/10], Train Loss: 0.4226, Train Acc: 0.8250, Val Loss: 0.2266, Val Acc: 0.9275, Test Loss: 0.4802, Test Acc: 0.7960\n","Epoch [10/10], Train Loss: 0.4010, Train Acc: 0.8225, Val Loss: 0.2209, Val Acc: 0.9125, Test Loss: 0.4824, Test Acc: 0.8109\n","PyTorch training loop with augmented data finished.\n"]}]},{"cell_type":"markdown","metadata":{"id":"635b06a5"},"source":["## Calculate Mean of Test Loss\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8a0d1cfc","executionInfo":{"status":"ok","timestamp":1764782704666,"user_tz":-180,"elapsed":7,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"}},"outputId":"aa05539c-e04f-4cb0-a6b6-77d1ad5bb73f"},"source":["mean_test_loss = np.mean(pytorch_history_augmented['test_loss'])\n","print(f\"Mean of test loss across augmented training epochs: {mean_test_loss:.4f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean of test loss across augmented training epochs: 0.5318\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6847f97e","executionInfo":{"status":"ok","timestamp":1764782956636,"user_tz":-180,"elapsed":35,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"}},"outputId":"675e7cdc-5895-4d9e-8e15-8cc9cb517454"},"source":["import numpy as np\n","\n","test_losses_augmented = pytorch_history_augmented['test_loss']\n","std_dev_test_loss_augmented = np.std(test_losses_augmented)\n","\n","print(f\"Standard deviation of TEST loss (with augmentations) for all epochs: {std_dev_test_loss_augmented:.4f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Standard deviation of TEST loss (with augmentations) for all epochs: 0.0759\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"67cdd196","executionInfo":{"status":"ok","timestamp":1764783111194,"user_tz":-180,"elapsed":16,"user":{"displayName":"brandon otieno","userId":"06691953118595767555"}},"outputId":"1fc53f1c-8a9b-4ebc-de39-0e526b3233a8"},"source":["import numpy as np\n","last_5_epochs_test_accuracy = pytorch_history_augmented['test_accuracy'][5:10]\n","\n","average_test_accuracy_last_5_epochs = np.mean(last_5_epochs_test_accuracy)\n","\n","print(f\"Average of test accuracy for the last 5 epochs (6 to 10) with augmentations: {average_test_accuracy_last_5_epochs:.4f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average of test accuracy for the last 5 epochs (6 to 10) with augmentations: 0.7821\n"]}]}]}